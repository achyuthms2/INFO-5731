{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unt-iialab/INFO5731_Spring2020/blob/master/Assignments/INFO5731_Assignment_Three.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9"
      },
      "source": [
        "# **INFO5731 Assignment Three**\n",
        "\n",
        "In this assignment, you are required to conduct information extraction, semantic analysis based on **the dataset you collected from assignment two**. You may use scipy and numpy package in this assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF"
      },
      "source": [
        "# **Question 1: Understand N-gram**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k"
      },
      "source": [
        "(45 points). Write a python program to conduct N-gram analysis based on the dataset in your assignment two:\n",
        "\n",
        "(1) Count the frequency of all the N-grams (N=3).\n",
        "\n",
        "(2) Calculate the probabilities for all the bigrams in the dataset by using the fomular count(w2 w1) / count(w2). For example, count(really like) / count(really) = 1 / 3 = 0.33.\n",
        "\n",
        "(3) Extract all the **noun phrases** and calculate the relative probabilities of each review in terms of other reviews (abstracts, or tweets) by using the fomular frequency (noun phrase) / max frequency (noun phrase) on the whole dataset. Print out the result in a table with column name the all the noun phrases and row name as all the 100 reviews (abstracts, or tweets). \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code here\n",
        "import nltk\n",
        "nltk.download()\n",
        "\n"
      ],
      "metadata": {
        "id": "L6snYljJ-6t2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PuFPKhC0m1fd"
      },
      "outputs": [],
      "source": [
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "!pip install selenium"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.support.ui import WebDriverWait as wait\n",
        "from selenium.webdriver.common.by import By\n",
        "import pandas as pd\n",
        "import time\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('-headless')\n",
        "options.add_argument('-no-sandbox')\n",
        "options.add_argument('-disable-dev-shm-usage')"
      ],
      "metadata": {
        "id": "dFVXYSdO_Bb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code here\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import requests\n",
        "import urllib.request\n",
        "  \n",
        "File = open(\"out.csv\", \"a\")\n",
        "  \n",
        "HEADERS = ({'User-Agent':\n",
        "           'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36',\n",
        "                           'Accept-Language': 'en-US, en;q=0.5'}) #This is the header with has the deatilas of the browers\n",
        "\n",
        "user_name=[]                # Initialising the empty lists\n",
        "review_title=[]\n",
        "ratings=[]\n",
        "review_whole_text=[]\n",
        "review_dates=[]\n",
        "user_name_final=[]\n",
        "review_dates_final=[]\n",
        "\n",
        "for number in range(1,20):\n",
        "  link= 'https://www.amazon.com/Apple-iPhone-11-64GB-Unlocked/product-reviews/B07ZPKF8RG/ref=cm_cr_arp_d_paging_btm_next_2?ie=UTF8&reviewerType=all_reviews&pageNumber='+ str(number) # By attaching the str(number) at the end the link generates dynamically\n",
        "  print(link)\n",
        "  webpage = requests.get(link, headers=HEADERS) # Using the .get method we are hitting the web url\n",
        "\n",
        "  soup = bs(webpage.content,'html.parser')    # Extracting the html content from the page \n",
        "  names= soup.find_all('span',class_='a-profile-name')  # Finding the class a_profile-name in the data which gives the results of all user names\n",
        "  \n",
        "  for i in range(2,len(names)):  # here we are iterating through all the names which consist of names and append tp the lsit\n",
        "    user_name.append(names[i].get_text())\n",
        "  \n",
        "  title = soup.find_all('a',class_='review-title-content') # Using the class name review tile content we are extracting the title data\n",
        " \n",
        "  for i in range(0,len(title)):\n",
        "    review_title.append(title[i].get_text())\n",
        "\n",
        "  review_title[:] = [titles.lstrip('\\n') for titles in review_title] # As the title  contains the \\n data here we are stripping using the methods\n",
        "  review_title[:] = [titles.rstrip('\\n') for titles in review_title]\n",
        "  # print(review_title)\n",
        "  # print(\"******review_title******\",len(review_title))\n",
        "  rating = soup.find_all('i',class_='review-rating') # Extarcting the review rating data usinf class review-rating\n",
        "  \n",
        "  for i in range(2,len(rating)):\n",
        "    ratings.append(rating[i].get_text())      #Appending it to the ratings list.\n",
        "  # print(\"*****ratings*****\",len(ratings))\n",
        "  review_text= soup.find_all('span',class_='review-text-content') # Extracting the review-text data using the class review-text-content\n",
        "  \n",
        "  for i in range(0,len(review_text)):\n",
        "    review_whole_text.append(review_text[i].get_text())  # Appending it to the review whole text list\n",
        "  review_whole_text[:] = [reviews.lstrip('\\n') for reviews in review_whole_text]  \n",
        "  review_whole_text[:] = [reviews.rstrip('\\n') for reviews in review_whole_text]\n",
        "  # print(review_whole_text)\n",
        "  # print(\"****review_whole_text******\",len(review_whole_text))\n",
        "  review_date = soup.find_all('span',class_='review-date')\n",
        "  \n",
        "  for i in range(2,len(review_date)):\n",
        "    review_dates.append(review_date[i].get_text())\n"
      ],
      "metadata": {
        "id": "FHdsXCG7_GhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df=pd.DataFrame()\n",
        "df['User Name']=pd.Series(user_name)     # Appending the list to the df column\n",
        "df['Review Title']=pd.Series(review_title)\n",
        "df['Review Text']=pd.Series(review_whole_text)\n",
        "df['Star']=pd.Series(ratings)\n",
        "df['Review posted time']=pd.Series(review_dates)\n",
        "print(df.head(150))\n",
        "df.to_csv(\"Iphone_reviews.csv\") "
      ],
      "metadata": {
        "id": "MHN0LNQU_Ns3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Lower Case\n",
        "Review_dataframe['After Converting to lower case'] = Review_dataframe['Review Text'].apply(lambda x: \" \".join(str(x).lower() for x in str(x).split()))\n",
        "Review_dataframe\n",
        "\n",
        "#Punctuation Removal\n",
        "Review_dataframe['After Removing Punctuation'] = Review_dataframe['After Converting to lower case'].str.replace('[^\\w\\s]','')\n",
        "Review_dataframe\n",
        "\n",
        "#Special Charachters Removal\n",
        "import re\n",
        "Review_dataframe['After Removing Special Charachters'] = Review_dataframe['After Removing Punctuation'].apply(lambda x: ''.join(re.sub(r\"[^a-zA-Z0-9]+\", ' ', charctr) for charctr in x ))\n",
        "Review_dataframe\n",
        "\n",
        "#Stopwords Removal\n",
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')\n",
        "Review_dataframe['After Removing Stopwords'] = Review_dataframe['After Removing Punctuation'].apply(lambda x: \" \".join(x for x in str(x).split() if x not in stop))\n",
        "Review_dataframe\n",
        "\n",
        "#Spelling Correction\n",
        "from textblob import TextBlob\n",
        "Review_dataframe['After Spelling Correction'] = Review_dataframe['After Removing Stopwords'].apply(lambda x: str(TextBlob(x).correct()))\n",
        "Review_dataframe\n",
        "\n",
        "#Tokenization\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "Review_dataframe['After Tokenization'] = Review_dataframe['After Spelling Correction'].apply(lambda x: TextBlob(x).words)\n",
        "Review_dataframe\n",
        "\n",
        "\n",
        "#Stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "Review_dataframe['After Stemming'] = Review_dataframe['After Tokenization'].apply(lambda x: \" \".join([ps.stem(word) for word in x]))\n",
        "Review_dataframe\n",
        "\n",
        "#Lemmatization\n",
        "from textblob import Word\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "Review_dataframe['After Lemmatization'] = Review_dataframe['After Stemming'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in str(x).split()]))\n",
        "Review_dataframe.to_csv('reviwes.csv',index=False)\n",
        "Review_dataframe\n"
      ],
      "metadata": {
        "id": "fgX-Bjja_RjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import collections\n",
        "from nltk import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "array= []\n",
        "for i in Review_dataframe['After Lemmatization']:\n",
        "  array.append(word_tokenize(i))\n",
        "cleaned_data = [x for x in array if x != []]\n",
        "repeat = list(itertools.chain.from_iterable(cleaned_data))\n",
        "\n",
        "trigrams = nltk.trigrams(repeat)\n",
        "FD = nltk.FreqDist(trigrams)\n",
        "FD"
      ],
      "metadata": {
        "id": "17MQ4wlE_UII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "complete_data = ''\n",
        "j= []\n",
        "count = 1\n",
        "for i in Review_dataframe['After Lemmatization']:\n",
        "  complete_data = complete_data + i\n",
        "  k = 'Review-' + str(i)\n",
        "  j.append(k)\n",
        "  count+= 1\n",
        "\n",
        "from textblob import TextBlob\n",
        "np = []\n",
        "fcy = []\n",
        "for i in Review_dataframe['After Lemmatization']:\n",
        "  blob = TextBlob(i)\n",
        "  for nouns in blob.noun_phrases:\n",
        "    np.append(nouns)\n",
        "for w in np:\n",
        "  npf = []\n",
        "  for i in Review_dataframe['After Lemmatization']:\n",
        "    npf.append(i.count(w) / complete_data.count(w))\n",
        "  fcy.append(np)\n",
        "noun_phrases_Data = pd.DataFrame(fcy).T\n",
        "noun_phrases_Data.columns = list(np)\n",
        "noun_phrases_Data.j = j\n",
        "noun_phrases_Data"
      ],
      "metadata": {
        "id": "8ICN0-LQ_XRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2: Undersand TF-IDF and Document representation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(20 points). Starting from the documents (all the reviews, or abstracts, or tweets) collected for assignment two, write a python program: \n",
        "\n",
        "(1) To build the **documents-terms weights (tf*idf) matrix bold text**.\n",
        "\n",
        "(2) To rank the documents with respect to query (design a query by yourself, for example, \"An Outstanding movie with a haunting performance and best character development\") by using **cosine similarity**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vATjQNTY8buA"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "sentences = Review_dataframe[\"After Spelling Correction\"].values.tolist()\n",
        "words = set([j for i in sentences for j in str(i).split(\" \")])\n",
        "tf_idf_df = pd.DataFrame(words, columns=[\"words\"])\n",
        "count=1\n",
        "\n",
        "def tf_idf(x,sentence):\n",
        "  value = sentence.count(x)\n",
        "  size = len(str(sentence).split(\" \"))\n",
        "  if(value != 0):\n",
        "    return (value/size)*(math.log(size/value, 10))\n",
        "  else:\n",
        "    return 0\n",
        "    \n",
        "for sentence in sentences:\n",
        "  tf_idf_df[\"Review\"+str(count)] = tf_idf_df[\"words\"].apply(lambda x: tf_idf(str(x),sentence))\n",
        "  count=count+1\n",
        "tf_idf_df\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "search_sentence=\"  Excellent shape, looks brand new and battery health is at 100%.  Came perfectly padded with charging cord and wall charger\"\n",
        "X_list = word_tokenize(search_sentence)\n",
        "stop_word = stopwords.words('english') \n",
        "X_set = {i for i in X_list if not i in stop_word}\n",
        "\n",
        "def cv(X_set, Y_set):\n",
        "  r_vector = X_set.union(Y_set)\n",
        "  list1 =[];list2 =[]\n",
        "  for j in r_vector: \n",
        "      if j in X_set: list1.append(1)\n",
        "      else: list1.append(0) \n",
        "      if j in Y_set: list2.append(1) \n",
        "      else: list2.append(0)\n",
        "  return r_vector, list1, list2\n",
        "\n",
        "def cc(r_vector, l1, l2):\n",
        "  count = 0\n",
        "  for i in range(len(r_vector)): \n",
        "        count+= list1[i]*list2[i] \n",
        "  c = count / float((sum(list1)*sum(list2))**0.5) \n",
        "  return c\n",
        "\n",
        "\n",
        "list3 = []\n",
        "for l in Review_dataframe['After Spelling Correction']:\n",
        "  Y_list = word_tokenize(l)\n",
        "  Y_set = {k for k in Y_list if not k in stop_word}\n",
        "  r_vector, list1, list2 = cv(X_set, Y_set)\n",
        "  try:\n",
        "    similarity = cc(r_vector, list1, list2)\n",
        "  except ZeroDivisionError:\n",
        "    similarity = 'None'\n",
        "  list3.append(similarity)\n",
        "Data2 = pd.DataFrame(list(zip(Review_dataframe['Review Text'],list3)), columns=['Review Text','cosine cimilarity'])\n",
        "Data2"
      ],
      "metadata": {
        "id": "l2eY1XZu_gJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cZosxyZ-yF0"
      },
      "source": [
        "# **Question 3: Create your own word embedding model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IVwpMLH-yF0"
      },
      "source": [
        "(20 points). Use the data you collected for assignment two to build a word embedding model: \n",
        "\n",
        "(1) Train a 300-dimension word embedding (it can be word2vec, glove, ulmfit, bert, or others).\n",
        "\n",
        "(2) Visualize the word embedding model you created.\n",
        "\n",
        "Reference: https://machinelearningmastery.com/develop-word-embeddings-python-gensim/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71D4VABp-yF0"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV"
      },
      "source": [
        "# **Question 4: Create your own training and evaluation data for sentiment analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(15 points). **You dodn't need to write program for this question!** Read each review (abstract or tweet) you collected in detail, and annotate each review with a sentiment (positive, negative, or neutral). Save the annotated dataset into a csv file with three columns (first column: document_id, clean_text, sentiment), upload the csv file to GitHub and submit the file link blew. This datset will be used for assignment four: sentiment analysis and text classification. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfvMKJjIXS5G"
      },
      "outputs": [],
      "source": [
        "# The GitHub link of your final csv file\n",
        "\n",
        " https://github.com/maddisurekha1234/surekha_INFO5731_-Fall2021/blob/main/reviwes.csv"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "INFO5731_Assignment_Three-1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}